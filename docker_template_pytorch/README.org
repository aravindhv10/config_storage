* Work space

** elisp stuff
#+begin_src emacs-lisp
  (save-buffer)
  (org-babel-tangle)
  (async-shell-command "./work.sh" "log" "error")
#+end_src

#+RESULTS:
: #<window 630 on log>

** shell stuff
#+begin_src sh :shebang #!/bin/sh :results output :tangle ./work.sh
  rm -vf -- 'README.org~' '.#README.org' './work.sh'
  git status
#+end_src

* Basic preparation

** Basic configs

*** Docker container names
#+begin_src sh :shebang #!/bin/sh :results output :tangle ./host.image_names.sh
  export IMAGE_NAME='jupyter_torch'
  export CONTAINER_NAME="${IMAGE_NAME}_1"

  docker_build(){
      sudo docker image build \
          -t "${IMAGE_NAME}"  \
          .                   \
      ;
  }
#+end_src

*** Script to build the docker image
#+begin_src sh :shebang #!/bin/sh :results output :tangle ./host.docker_build.sh
  cd "$('dirname' '--' "${0}")"
  . './host.image_names.sh'
#+end_src

*** Main base image
#+begin_src conf :tangle ./Dockerfile
  # FROM pytorch/pytorch:2.3.1-cuda12.1-cudnn8-devel
  # FROM pytorch/pytorch:2.3.0-cuda12.1-cudnn8-devel
  # FROM pytorch/pytorch:2.3.1-cuda11.8-cudnn8-devel
  FROM nvidia/cuda:12.4.1-cudnn-devel-ubuntu22.04 as cuda_deb
#+end_src

*** Basic configs
#+begin_src conf :tangle ./Dockerfile
  ENV HOME='/root'
  ENV DEBIAN_FRONTEND='noninteractive'
  WORKDIR '/root'
#+end_src

** Important apt install stuff
#+begin_src conf :tangle ./Dockerfile
  RUN \
      --mount=target=/var/lib/apt/lists,type=cache,sharing=locked \
      --mount=target=/var/cache/apt,type=cache,sharing=locked \
      echo 'Starting apt-get stuff' \
      && apt-get -y update \
      && apt-get install -y \
          aria2 \
          bash \
          build-essential \
          ca-certificates \
          curl \
          git \
          git-lfs \
          libgl1 \
          libglib2.0-0 \
          libsm6 \
          libsndfile1-dev \
          libxext6 \
          libxrender1 \
          neovim \
          python3-dev \
          python3-pip \
      && echo 'Done apt-get stuff' ;
#+end_src

** Build the docker image
#+begin_src sh :shebang #!/bin/sh :results output :tangle ./host.docker_build.sh
  docker_build
#+end_src

* Installing UV

** Next stage of build
#+begin_src conf :tangle ./Dockerfile
  FROM cuda_deb as cuda_deb_uv
#+end_src

** Install uv
#+begin_src conf :tangle ./Dockerfile
  RUN \
      echo 'Starting uv download' \
      && curl -LsSf 'https://astral.sh/uv/install.sh' | sh \
      && cp -vf -- "${HOME}/.local/bin/uv" '/usr/local/bin/' \
      && echo 'Done uv download' ;
#+end_src

** start venv
#+begin_src conf :tangle ./Dockerfile
  RUN \
      echo 'starting' \
      && uv venv "${HOME}/venv" \
      && echo 'done' ;
#+end_src

* Installing essential deep learning libraries

** pip stage of the build
#+begin_src conf :tangle ./Dockerfile
  FROM cuda_deb_uv as cuda_deb_uv_pip
#+end_src

** Install pypi stuff

*** Basic stuff
#+begin_src conf :tangle ./Dockerfile
  RUN \
      echo 'starting' \
      && . "${HOME}/venv/bin/activate" \
      && uv pip install -U \
          pip \
          setuptools \
          wheel \
      && echo 'done' ;
#+end_src

*** Torch stuff
#+begin_src conf :tangle ./Dockerfile
  RUN \
      echo 'starting' \
      && . "${HOME}/venv/bin/activate" \
      && uv pip install \
          torch \
          torchao \
          torchaudio \
          torchvision \
      && echo 'done' ;
#+end_src

*** Extra libraries
#+begin_src conf :tangle ./Dockerfile
  RUN \
      echo 'starting' \
      && . "${HOME}/venv/bin/activate" \
      && uv pip install \
          accelerate \
          deepspeed \
          diffusers \
          einops \
          huggingface-hub \
          inotify-simple \
          ninja \
          optimum-quanto \
          packaging \
          peft \
          prodigyopt \
          sentencepiece \
          transformers \
      && echo 'done' ;
#+end_src

*** More Extra libraries
#+begin_src conf :tangle ./Dockerfile
  RUN \
      echo 'starting' \
      && . /root/venv/bin/activate \
      && uv pip install \
          accelerate \
          datasets \
          decord \
          deepspeed \
          diffusers \
          einops \
          gekko \
          huggingface-hub \
          inotify-simple \
          ninja \
          optimum-quanto \
          packaging \
          peft \
          prodigyopt \
          protobuf \
          qwen-vl-utils \
          sentencepiece \
          transformers \
      && echo 'done' ;
#+end_src

*** quantization
#+begin_src conf :tangle ./Dockerfile
  RUN \
      echo 'starting' \
      && . "${HOME}/venv/bin/activate" \
      && uv pip install --no-deps \
          autoawq-kernels \
          auto-gptq \
          autoawq \
          optimum  \
      && echo 'done' ;
#+end_src

** Clone and install from source

*** Transformers
#+begin_src conf :tangle ./Dockerfile
  RUN \
      echo 'starting' \
      && cd /root \
      && git clone --depth 1 'https://github.com/huggingface/transformers.git' \
      && cd transformers \
      && . /root/venv/bin/activate \
      && uv pip install -e . \
      && echo 'done' ;
#+end_src

*** Diffusers
#+begin_src conf :tangle ./Dockerfile
  RUN \
      echo 'starting' \
      && cd /root \
      && git clone --depth 1 'https://github.com/huggingface/diffusers.git' \
      && cd diffusers \
      && . /root/venv/bin/activate \
      && uv pip install -e . \
      && echo 'done' ;
#+end_src

* flash attn part

** Inheriting from previous section
#+begin_src conf :tangle ./Dockerfile
  FROM cuda_deb_uv_pip as cuda_deb_uv_pip_flash
#+end_src

** flash attn

*** COMMENT using uv
#+begin_src conf :tangle ./Dockerfile
  RUN \
      echo 'starting' \
      && . /root/venv/bin/activate \
      && uv pip install --no-build-isolation \
          flash-attn \
      && echo 'done' ;
#+end_src

*** using pip
#+begin_src conf :tangle ./Dockerfile
  RUN \
      echo 'starting' \
      && . /root/venv/bin/activate \
      && pip3 install \
          flash-attn \
      && echo 'done' ;
#+end_src

** Install xformers
#+begin_src conf :tangle ./Dockerfile
  RUN \
      echo 'starting' \
      && . /root/venv/bin/activate \
      && uv pip install \
          xformers \
      && echo 'done' ;
#+end_src

* Image for jupyter

** Main docker image
#+begin_src conf :tangle ./Dockerfile
  FROM cuda_deb_uv_pip_flash as cuda_deb_uv_pip_flash_jupyter
#+end_src

** jupyter lab
#+begin_src conf :tangle ./Dockerfile
  RUN \
      echo 'starting' \
      && . "${HOME}/venv/bin/activate" \
      && uv pip install \
          jupyterlab \
      && echo 'done' ;
#+end_src

** Expose the jupyterlab port
#+begin_src conf :tangle ./Dockerfile
  EXPOSE 8888/tcp
#+end_src

** Script to start jupyterlab server

*** Copy the script
#+begin_src conf :tangle ./Dockerfile
  COPY './docker.start_jupyter_lab.sh' '/root/docker.start_jupyter_lab.sh'
#+end_src

*** Main shell script for starting jupyterlab
#+begin_src sh :shebang #!/bin/sh :results output :tangle ./docker.start_jupyter_lab.sh
  cd '/data/output'
  cd "${HOME}"
  . "${HOME}/venv/bin/activate"
  exec 'jupyter' 'lab' '--allow-root' '--ip=0.0.0.0'
#+end_src

* Script to run the docker iamge

** List of command line arguments
#+begin_src sh :shebang #!/bin/sh :results output :tangle ./host.docker_run_interactive.txt
  --tty
  --interactive
  --rm
  --gpus all
  --ipc host                                                           
  --ulimit memlock=-1                                                  
  --ulimit stack=67108864                                              
  --mount 'type=tmpfs,destination=/data/TMPFS,tmpfs-size=137438953472' 
  -p '0.0.0.0:8888:8888/tcp'                                           
  -v "CACHE:/root/.cache"                                              
  -v "${INPUT}:/data/input"                                            
  -v "${OUTPUT}:/data/output"                                          
  "${IMAGE_NAME}"                                                      
  '/bin/bash'                                                          
#+end_src

** Main script
#+begin_src sh :shebang #!/bin/sh :results output :tangle ./host.docker_run_interactive.sh
  mkdir -pv -- "${2}"

  export INPUT="$(realpath -- "${1}")"
  export OUTPUT="$(realpath -- "${2}")"

  cd "$('dirname' '--' "${0}")"

  . './host.image_names.sh'

  sudo docker run                                                          \
      --tty                                                                \
      --interactive                                                        \
      --rm                                                                 \
      --gpus all                                                           \
      --ipc host                                                           \
      --ulimit memlock=-1                                                  \
      --ulimit stack=67108864                                              \
      --mount 'type=tmpfs,destination=/data/TMPFS,tmpfs-size=137438953472' \
      -p '0.0.0.0:8888:8888/tcp'                                           \
      -v "CACHE:/root/.cache"                                              \
      -v "${INPUT}:/data/input"                                            \
      -v "${OUTPUT}:/data/output"                                          \
      "${IMAGE_NAME}"                                                      \
      '/bin/bash'                                                          \
  ;
#+end_src

* Inference image for large models

** The image declaration
#+begin_src conf :tangle ./Dockerfile
  FROM cuda_deb_uv_pip_flash_jupyter as cuda_deb_uv_pip_flash_jupyter_inference
#+end_src

** Huggingface accelerate config

*** Copy the file into the image
#+begin_src conf :tangle ./Dockerfile
  COPY './default_config.yaml' '/root/default_config.yaml'
#+end_src

*** Actual file which seems to be working with qwen 2 VL 72B
#+begin_src conf :tangle ./default_config.yaml
  compute_environment: LOCAL_MACHINE
  debug: false
  deepspeed_config:
    gradient_accumulation_steps: 1
    offload_optimizer_device: cpu
    offload_param_device: cpu
    zero3_init_flag: true
    zero3_save_16bit_model: true
    zero_stage: 3
  distributed_type: DEEPSPEED
  downcast_bf16: 'no'
  dynamo_config:
    dynamo_backend: INDUCTOR
  enable_cpu_affinity: false
  machine_rank: 0
  main_training_function: main
  mixed_precision: bf16
  num_machines: 1
  num_processes: 1
  rdzv_backend: static
  same_network: true
  tpu_env: []
  tpu_use_cluster: false
  tpu_use_sudo: false
  use_cpu: false
#+end_src

** Inference scripts

*** QWEN 2 VL

**** python
#+begin_src python :shebang #!/usr/bin/python3 :results output :tangle ./docker.infer_qwen.py
  from qwen_vl_utils import process_vision_info
  from transformers import AutoProcessor
  from transformers import AutoTokenizer
  from transformers import Qwen2VLForConditionalGeneration
  import os
  import sys
  import time
  import torch


  def remove_extension(path_input):
      loc = path_input.rfind(".")
      return path_input[0:loc]


  def get_all_images(path_dir_input):
      ret = []

      for dirpath, dirnames, filenames in os.walk(path_dir_input):
          for filename in filenames:
              tmp = filename.lower()

              if tmp.endswith(".jpg") or tmp.endswith(".jpeg") or tmp.endswith(".png"):
                  ret.append(os.path.join(dirpath, filename))

      return ret


  def replace_base_dir(list_paths, path_input, path_output):
      res = list(path_output + i[len(path_input) :] for i in list_paths)
      return res


  class infer_slave:
      def __init__(self, model_index=0):
          model_list = (
              "Qwen/Qwen2-VL-72B-Instruct-GPTQ-Int8",
              "Qwen/Qwen2-VL-7B-Instruct-GPTQ-Int8",
              "Qwen/Qwen2-VL-7B-Instruct-AWQ",
              "Qwen/Qwen2-VL-7B-Instruct",
          )

          self.model_name = model_list[model_index]

          self.model = Qwen2VLForConditionalGeneration.from_pretrained(
              self.model_name,
              torch_dtype=torch.bfloat16,
              attn_implementation="flash_attention_2",
              device_map="auto",
          )

          self.processor = AutoProcessor.from_pretrained(
              self.model_name,
          )

      def do_process(self, path_image_input, path_caption_input):
          messages = [
              {
                  "role": "user",
                  "content": [
                      {
                          "type": "image",
                          "image": path_image_input,
                      },
                      {
                          "type": "text",
                          "text": open(path_caption_input, "r", encoding="utf-8").read(),
                      },
                  ],
              }
          ]

          # Preparation for inference
          text = self.processor.apply_chat_template(
              messages, tokenize=False, add_generation_prompt=True
          )

          image_inputs, video_inputs = process_vision_info(messages)

          inputs = self.processor(
              text=[text],
              images=image_inputs,
              videos=video_inputs,
              padding=True,
              return_tensors="pt",
          )

          inputs = inputs.to("cuda")
          return inputs

      def do_infer(self, path_image_input, path_caption_input):
          messages = [
              {
                  "role": "user",
                  "content": [
                      {
                          "type": "image",
                          "image": path_image_input,
                      },
                      {
                          "type": "text",
                          "text": open(path_caption_input, "r", encoding="utf-8").read(),
                      },
                  ],
              }
          ]

          # Preparation for inference
          text = self.processor.apply_chat_template(
              messages, tokenize=False, add_generation_prompt=True
          )

          image_inputs, video_inputs = process_vision_info(messages)

          inputs = self.processor(
              text=[text],
              images=image_inputs,
              videos=video_inputs,
              padding=True,
              return_tensors="pt",
          )

          inputs = inputs.to("cuda")

          # Inference: Generation of the output
          generated_ids = self.model.generate(**inputs, max_new_tokens=1024)
          generated_ids_trimmed = [
              out_ids[len(in_ids) :]
              for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
          ]
          output_text = self.processor.batch_decode(
              generated_ids_trimmed,
              skip_special_tokens=True,
              clean_up_tokenization_spaces=False,
          )

          os.unlink(path_image_input)
          os.unlink(path_caption_input)

          return output_text

      def do_docker_infer(self):
          list_path_images = get_all_images(path_dir_input="/data/input")
          list_path_images.sort()

          list_path_captions = list(
              remove_extension(path_input=i) + ".txt" for i in list_path_images
          )

          list_path_work = list(
              remove_extension(path_input=i) + ".work" for i in list_path_images
          )

          list_path_captions_output = replace_base_dir(
              list_paths=list_path_captions,
              path_input="/data/input",
              path_output="/data/output",
          )

          for i in range(len(list_path_images)):
              path_done = (
                  remove_extension(path_input=list_path_captions_output[i]) + ".done"
              )

              if (
                  os.path.exists(list_path_captions[i])
                  and os.path.exists(list_path_work[i])
                  and (not os.path.exists(path_done))
              ):
                  if os.path.exists(list_path_captions_output[i]):
                      os.unlink(list_path_captions_output[i])

                  res = self.do_infer(
                      path_image_input=list_path_images[i],
                      path_caption_input=list_path_captions[i],
                  )[0]

                  open(list_path_captions_output[i], "w", encoding="utf-8").write(res)

                  os.unlink(list_path_work[i])

                  open(path_done, "w").close()


  slave = infer_slave()
  slave.do_docker_infer()

  while len(sys.argv) > 1:
      time.sleep(0.2)
      slave.do_docker_infer()
#+end_src

**** shell
#+begin_src sh :shebang #!/bin/sh :results output :tangle ./docker.infer_qwen.sh
  cd "${HOME}"

  . "${HOME}/venv/bin/activate"

  cp -vf -- \
      "${HOME}/default_config.yaml" \
      "${HOME}/.cache/huggingface/accelerate/default_config.yaml" ;

  accelerate launch "${HOME}/docker.infer_qwen.py"
#+end_src

**** Copy the inference script into docker
#+begin_src conf :tangle ./Dockerfile
  COPY './docker.infer_qwen.py' '/root/docker.infer_qwen.py'
  COPY './docker.infer_qwen.sh' '/root/docker.infer_qwen.sh'
#+end_src

*** FLUX

**** python
#+begin_src python :shebang #!/usr/bin/python3 :results output :tangle ./docker.infer_flux.py
  import torch
  from diffusers import FluxPipeline

  pipe = FluxPipeline.from_pretrained(
      "black-forest-labs/FLUX.1-dev", device_map="balanced", torch_dtype=torch.bfloat16
  )

  pipe.transformer = torch.compile(pipe.transformer)


  def do_infer(prompt, path_image_output, width=1360, height=768):
      out = pipe(
          prompt=prompt,
          guidance_scale=3.5,
          height=height,
          width=width,
          num_inference_steps=20,
      ).images[0]

      out.save(path_image_output)


  prompt = "a tiny astronaut hatching from an egg on the moon"
  do_infer(
      prompt=prompt, path_image_output="/data/output/image.png", width=1360, height=768
  )
#+end_src

**** shell
#+begin_src sh :shebang #!/bin/sh :results output :tangle ./docker.infer_flux.sh
  cd "${HOME}"

  . "${HOME}/venv/bin/activate"

  cp -vf -- \
      "${HOME}/default_config.yaml" \
      "${HOME}/.cache/huggingface/accelerate/default_config.yaml" ;

  accelerate launch "${HOME}/docker.infer_flux.py"
#+end_src

**** Copy the inference script into docker
#+begin_src conf :tangle ./Dockerfile
  COPY './docker.infer_flux.py' '/root/docker.infer_flux.py'
  COPY './docker.infer_flux.sh' '/root/docker.infer_flux.sh'
#+end_src

* Final inference

** The image declaration
#+begin_src conf :tangle ./Dockerfile
  FROM cuda_deb_uv_pip_flash_jupyter_inference
#+end_src
