* Work space

** elisp stuff
#+begin_src emacs-lisp
  (save-buffer)
  (org-babel-tangle)
  (async-shell-command "./work.sh" "log" "error")
#+end_src

#+RESULTS:
: #<window 2004 on log>

** shell stuff
#+begin_src sh :shebang #!/bin/sh :results output :tangle ./work.sh
  rm -vf -- 'README.org~' '.#README.org' './work.sh'
  git status
#+end_src


* MOA prompt
You have been provided with an image and a set of descriptions of the image from various open-source models. Your task is to synthesize these descriptions into a single, high-quality description of the given image. It is crucial to critically evaluate the information provided in these descriptions, recognizing that some of them may be incomplete, biased or incorrect. Your description should not simply replicate the given descriptions but should offer a refined, accurate, and comprehensive reply. Ensure your response is well-structured, coherent, and adheres to the highest standards of accuracy and reliability.

Responses from models:
1. The image depicts a whimsical scene set on the moon's surface, characterized by its gray, cratered terrain and a dark, star-studded sky. At the center of the image, a small, white, fuzzy creature, resembling a bunny, is hatching from a brown egg. The creature is adorned in a space suit with a black visor, giving it a unique, otherworldly appearance. The egg is cracked open, with its shell split in half, revealing the creature emerging. The background is blurred, emphasizing the creature and the egg, while the moon's surface is detailed with craters and small rocks. The overall composition creates a surreal and imaginative portrayal of a space bunny hatching on the moon.
2. The image depicts a small, plush astronaut toy emerging from an eggshell on a rocky, moon-like surface. The background is dark with a few distant lights, giving the impression of a space or lunar environment. The scene is illuminated by a soft light, highlighting the texture of the eggshell and the details of the astronaut toy. The overall atmosphere is whimsical and imaginative, blending elements of space exploration with a playful, fantastical twist.âŽ     ubuntu@aravind-ml-a100

Result: The image presents a whimsical and imaginative scene set on a moon-like surface, characterized by its gray, cratered terrain and a dark, star-studded sky. At the center of the image, a small, white, fuzzy creature resembling a bunny is emerging from a brown egg. The creature is dressed in a space suit with a black visor, adding a unique, otherworldly touch to its appearance. The egg is cracked open, with its shell split in half, revealing the creature as it hatches. The background is softly blurred, drawing focus to the creature and the egg, while the moon's surface is detailed with craters and small rocks. The overall composition creates a surreal and playful portrayal of a space bunny hatching on the moon, blending elements of space exploration with a fantastical twist.

* Basic preparation

** Basic configs

*** Docker container names
#+begin_src sh :shebang #!/bin/sh :results output :tangle ./host.image_names.sh
  export IMAGE_NAME='jupyter_torch'
  export CONTAINER_NAME="${IMAGE_NAME}_1"

  docker_build(){
      sudo docker image build \
          -t "${IMAGE_NAME}"  \
          .                   \
      ;
  }
#+end_src

*** Script to build the docker image
#+begin_src sh :shebang #!/bin/sh :results output :tangle ./host.docker_build.sh
  cd "$('dirname' '--' "${0}")"
  . './host.image_names.sh'
#+end_src

*** Main base image
#+begin_src conf :tangle ./Dockerfile
  # FROM pytorch/pytorch:2.3.1-cuda12.1-cudnn8-devel
  # FROM pytorch/pytorch:2.3.0-cuda12.1-cudnn8-devel
  # FROM pytorch/pytorch:2.3.1-cuda11.8-cudnn8-devel
  FROM nvidia/cuda:12.4.1-cudnn-devel-ubuntu22.04 AS cuda_deb
#+end_src

*** Basic configs
#+begin_src conf :tangle ./Dockerfile
  ENV HOME='/root'
  ENV DEBIAN_FRONTEND='noninteractive'
  WORKDIR '/root'
#+end_src

** Important apt install stuff
#+begin_src conf :tangle ./Dockerfile
  RUN \
      --mount=target=/var/lib/apt/lists,type=cache,sharing=locked \
      --mount=target=/var/cache/apt,type=cache,sharing=locked \
      echo 'Starting apt-get stuff' \
      && apt-get -y update \
      && apt-get install -y \
          aria2 \
          bash \
          build-essential \
          ca-certificates \
          curl \
          git \
          git-lfs \
          libgl1 \
          libglib2.0-0 \
          libsm6 \
          libsndfile1-dev \
          libxext6 \
          libxrender1 \
          neovim \
          python3-dev \
          python3-pip \
      && echo 'Done apt-get stuff' ;
#+end_src

** Build the docker image
#+begin_src sh :shebang #!/bin/sh :results output :tangle ./host.docker_build.sh
  docker_build
#+end_src

* Script to run the docker iamge

** Main script header
#+begin_src sh :shebang #!/bin/sh :results output :tangle ./host.docker_run_interactive.sh
  tail -n +5 "${0}" | tr '\n' ' ' > "${0}.slave.sh"
  exec sh "${0}.slave.sh" "${1}" "${2}"
  exit
#+end_src

** Main script wrapper for preparing
#+begin_src sh :shebang #!/bin/sh :results output :tangle ./host.docker_run_interactive.sh
  mkdir -pv -- "${2}" ;
  export INPUT="$(realpath -- "${1}")" ;
  export OUTPUT="$(realpath -- "${2}")" ;
  cd "$('dirname' '--' "${0}")" ;
  . './host.image_names.sh' ;
#+end_src

** Main script wrapper for docker run
#+begin_src sh :shebang #!/bin/sh :results output :tangle ./host.docker_run_interactive.sh
  sudo docker run
  --tty
  --interactive
  --rm
  --gpus all
  --ipc host
  --ulimit memlock=-1
  --ulimit stack=67108864
#+end_src

** Main script wrapper for all mounts
#+begin_src sh :shebang #!/bin/sh :results output :tangle ./host.docker_run_interactive.sh
  --mount 'type=tmpfs,destination=/data/TMPFS,tmpfs-size=137438953472'
  -v "${INPUT}:/data/input"
  -v "${OUTPUT}:/data/output"
#+end_src

** Main script for mounting the cache
#+begin_src sh :shebang #!/bin/sh :results output :tangle ./host.docker_run_interactive.sh
  -v "CACHE:/root/.cache"
#+end_src

* Installing UV

** Next stage of build
#+begin_src conf :tangle ./Dockerfile
  FROM cuda_deb AS cuda_deb_uv
#+end_src

** Install uv
#+begin_src conf :tangle ./Dockerfile
  RUN \
      echo 'Starting uv download' \
      && curl -LsSf 'https://astral.sh/uv/install.sh' | sh \
      && cp -vf -- "${HOME}/.local/bin/uv" '/usr/local/bin/' \
      && echo 'Done uv download' ;
#+end_src

** start venv
#+begin_src conf :tangle ./Dockerfile
  RUN \
      echo 'starting' \
      && uv venv "${HOME}/venv" \
      && echo 'done' ;
#+end_src

* Installing essential deep learning libraries

** pip stage of the build
#+begin_src conf :tangle ./Dockerfile
  FROM cuda_deb_uv AS cuda_deb_uv_pip
#+end_src

** Install pypi stuff

*** Basic stuff
#+begin_src conf :tangle ./Dockerfile
  RUN \
      echo 'starting' \
      && . "${HOME}/venv/bin/activate" \
      && uv pip install -U \
          pip \
          setuptools \
          wheel \
      && echo 'done' ;
#+end_src

*** Torch stuff
#+begin_src conf :tangle ./Dockerfile
  RUN \
      echo 'starting' \
      && . "${HOME}/venv/bin/activate" \
      && uv pip install \
          torch \
          torchao \
          torchaudio \
          torchvision \
      && echo 'done' ;
#+end_src

*** Extra libraries
#+begin_src conf :tangle ./Dockerfile
  RUN \
      echo 'starting' \
      && . "${HOME}/venv/bin/activate" \
      && uv pip install \
          accelerate \
          deepspeed \
          diffusers \
          einops \
          huggingface-hub \
          inotify-simple \
          ninja \
          optimum-quanto \
          packaging \
          peft \
          prodigyopt \
          sentencepiece \
          transformers \
      && echo 'done' ;
#+end_src

*** More Extra libraries
#+begin_src conf :tangle ./Dockerfile
  RUN \
      echo 'starting' \
      && . /root/venv/bin/activate \
      && uv pip install \
          accelerate \
          datasets \
          decord \
          deepspeed \
          diffusers \
          einops \
          gekko \
          huggingface-hub \
          inotify-simple \
          ninja \
          optimum-quanto \
          packaging \
          peft \
          prodigyopt \
          protobuf \
          qwen-vl-utils \
          sentencepiece \
          transformers \
      && echo 'done' ;
#+end_src

*** quantization
#+begin_src conf :tangle ./Dockerfile
  RUN \
      echo 'starting' \
      && . "${HOME}/venv/bin/activate" \
      && uv pip install --no-deps \
          autoawq-kernels \
          auto-gptq \
          autoawq \
          optimum  \
      && echo 'done' ;
#+end_src

** Clone and install from source

*** Transformers
#+begin_src conf :tangle ./Dockerfile
  RUN \
      echo 'starting' \
      && cd /root \
      && git clone --depth 1 'https://github.com/huggingface/transformers.git' \
      && cd transformers \
      && . /root/venv/bin/activate \
      && uv pip install -e . \
      && echo 'done' ;
#+end_src

*** Diffusers
#+begin_src conf :tangle ./Dockerfile
  RUN \
      echo 'starting' \
      && cd /root \
      && git clone --depth 1 'https://github.com/huggingface/diffusers.git' \
      && cd diffusers \
      && . /root/venv/bin/activate \
      && uv pip install -e . \
      && echo 'done' ;
#+end_src

* flash attn part

** Inheriting from previous section
#+begin_src conf :tangle ./Dockerfile
  FROM cuda_deb_uv_pip AS cuda_deb_uv_pip_flash
#+end_src

** flash attn

*** COMMENT using uv
#+begin_src conf :tangle ./Dockerfile
  RUN \
      echo 'starting' \
      && . /root/venv/bin/activate \
      && uv pip install --no-build-isolation \
          flash-attn \
      && echo 'done' ;
#+end_src

*** using pip
#+begin_src conf :tangle ./Dockerfile
  RUN \
      echo 'starting' \
      && . /root/venv/bin/activate \
      && pip3 install \
          flash-attn \
      && echo 'done' ;
#+end_src

** Install xformers
#+begin_src conf :tangle ./Dockerfile
  RUN \
      echo 'starting' \
      && . /root/venv/bin/activate \
      && uv pip install \
          xformers \
      && echo 'done' ;
#+end_src

* Image for jupyter

** Main docker image
#+begin_src conf :tangle ./Dockerfile
  FROM cuda_deb_uv_pip_flash AS cuda_deb_uv_pip_flash_jupyter
#+end_src

** jupyter lab
#+begin_src conf :tangle ./Dockerfile
  RUN \
      echo 'starting' \
      && . "${HOME}/venv/bin/activate" \
      && uv pip install \
          ipywidgets \
          jupyterlab \
      && echo 'done' ;
#+end_src

** Expose the jupyterlab port

*** Inside the container
#+begin_src conf :tangle ./Dockerfile
  EXPOSE 8888/tcp
#+end_src

*** During docker run
#+begin_src sh :shebang #!/bin/sh :results output :tangle ./host.docker_run_interactive.sh
  -p '0.0.0.0:8888:8888/tcp'
#+end_src

** Script to start jupyterlab server

*** Copy the script
#+begin_src conf :tangle ./Dockerfile
  COPY './docker.start_jupyter_lab.sh' '/root/docker.start_jupyter_lab.sh'
#+end_src

*** Main shell script for starting jupyterlab
#+begin_src sh :shebang #!/bin/sh :results output :tangle ./docker.start_jupyter_lab.sh
  cd '/data/output'
  . "${HOME}/venv/bin/activate"
  exec 'jupyter' 'lab' '--allow-root' '--ip=0.0.0.0'
#+end_src

* Inference image for large models

** The image declaration
#+begin_src conf :tangle ./Dockerfile
  FROM cuda_deb_uv_pip_flash_jupyter AS cuda_deb_uv_pip_flash_jupyter_inference
#+end_src

** Huggingface accelerate config

*** Copy the file into the image
#+begin_src conf :tangle ./Dockerfile
  COPY './default_config.yaml' '/root/default_config.yaml'
#+end_src

*** Actual file which seems to be working with qwen 2 VL 72B
#+begin_src conf :tangle ./default_config.yaml
  compute_environment: LOCAL_MACHINE
  debug: false
  deepspeed_config:
    gradient_accumulation_steps: 1
    offload_optimizer_device: cpu
    offload_param_device: cpu
    zero3_init_flag: true
    zero3_save_16bit_model: true
    zero_stage: 3
  distributed_type: DEEPSPEED
  downcast_bf16: 'no'
  dynamo_config:
    dynamo_backend: INDUCTOR
  enable_cpu_affinity: false
  machine_rank: 0
  main_training_function: main
  mixed_precision: bf16
  num_machines: 1
  num_processes: 1
  rdzv_backend: static
  same_network: true
  tpu_env: []
  tpu_use_cluster: false
  tpu_use_sudo: false
  use_cpu: false
#+end_src

** Inference scripts

*** QWEN 2 VL

**** python
#+begin_src python :shebang #!/usr/bin/python3 :results output :tangle ./docker.infer_qwen.py
  from qwen_vl_utils import process_vision_info
  from transformers import AutoProcessor
  from transformers import AutoTokenizer
  from transformers import Qwen2VLForConditionalGeneration
  import os
  import sys
  import time
  import torch


  def remove_extension(path_input):
      loc = path_input.rfind(".")
      return path_input[0:loc]


  def get_all_images(path_dir_input):
      ret = []

      for dirpath, dirnames, filenames in os.walk(path_dir_input):
          for filename in filenames:
              tmp = filename.lower()

              if tmp.endswith(".jpg") or tmp.endswith(".jpeg") or tmp.endswith(".png"):
                  ret.append(os.path.join(dirpath, filename))

      return ret


  def replace_base_dir(list_paths, path_input, path_output):
      res = list(path_output + i[len(path_input) :] for i in list_paths)
      return res


  class infer_slave:
      def __init__(self, model_index=0):
          model_list = (
              "Qwen/Qwen2-VL-72B-Instruct-GPTQ-Int8",
              "Qwen/Qwen2-VL-7B-Instruct-GPTQ-Int8",
              "Qwen/Qwen2-VL-7B-Instruct-AWQ",
              "Qwen/Qwen2-VL-7B-Instruct",
          )

          self.model_name = model_list[model_index]

          self.model = Qwen2VLForConditionalGeneration.from_pretrained(
              self.model_name,
              torch_dtype=torch.bfloat16,
              attn_implementation="flash_attention_2",
              device_map="auto",
          )

          self.processor = AutoProcessor.from_pretrained(
              self.model_name,
          )

      def do_process(self, path_image_input, path_caption_input):
          messages = [
              {
                  "role": "user",
                  "content": [
                      {
                          "type": "image",
                          "image": path_image_input,
                      },
                      {
                          "type": "text",
                          "text": open(path_caption_input, "r", encoding="utf-8").read(),
                      },
                  ],
              }
          ]

          # Preparation for inference
          text = self.processor.apply_chat_template(
              messages, tokenize=False, add_generation_prompt=True
          )

          image_inputs, video_inputs = process_vision_info(messages)

          inputs = self.processor(
              text=[text],
              images=image_inputs,
              videos=video_inputs,
              padding=True,
              return_tensors="pt",
          )

          inputs = inputs.to("cuda")
          return inputs

      def do_infer(self, path_image_input, path_caption_input):
          messages = [
              {
                  "role": "user",
                  "content": [
                      {
                          "type": "image",
                          "image": path_image_input,
                      },
                      {
                          "type": "text",
                          "text": open(path_caption_input, "r", encoding="utf-8").read(),
                      },
                  ],
              }
          ]

          # Preparation for inference
          text = self.processor.apply_chat_template(
              messages, tokenize=False, add_generation_prompt=True
          )

          image_inputs, video_inputs = process_vision_info(messages)

          inputs = self.processor(
              text=[text],
              images=image_inputs,
              videos=video_inputs,
              padding=True,
              return_tensors="pt",
          )

          inputs = inputs.to("cuda")

          # Inference: Generation of the output
          generated_ids = self.model.generate(**inputs, max_new_tokens=1024)
          generated_ids_trimmed = [
              out_ids[len(in_ids) :]
              for in_ids, out_ids in zip(inputs.input_ids, generated_ids)
          ]
          output_text = self.processor.batch_decode(
              generated_ids_trimmed,
              skip_special_tokens=True,
              clean_up_tokenization_spaces=False,
          )

          os.unlink(path_image_input)
          os.unlink(path_caption_input)

          return output_text

      def do_docker_infer(self):
          list_path_images = get_all_images(path_dir_input="/data/input")
          list_path_images.sort()

          list_path_captions = list(
              remove_extension(path_input=i) + ".txt" for i in list_path_images
          )

          list_path_work = list(
              remove_extension(path_input=i) + ".work" for i in list_path_images
          )

          list_path_captions_output = replace_base_dir(
              list_paths=list_path_captions,
              path_input="/data/input",
              path_output="/data/output",
          )

          for i in range(len(list_path_images)):
              path_done = (
                  remove_extension(path_input=list_path_captions_output[i]) + ".done"
              )

              if (
                  os.path.exists(list_path_captions[i])
                  and os.path.exists(list_path_work[i])
                  and (not os.path.exists(path_done))
              ):
                  if os.path.exists(list_path_captions_output[i]):
                      os.unlink(list_path_captions_output[i])

                  res = self.do_infer(
                      path_image_input=list_path_images[i],
                      path_caption_input=list_path_captions[i],
                  )[0]

                  open(list_path_captions_output[i], "w", encoding="utf-8").write(res)

                  os.unlink(list_path_work[i])

                  open(path_done, "w").close()


  slave = infer_slave()
  slave.do_docker_infer()

  while len(sys.argv) > 1:
      time.sleep(0.2)
      slave.do_docker_infer()
#+end_src

**** shell
#+begin_src sh :shebang #!/bin/sh :results output :tangle ./docker.infer_qwen.sh
  cd "${HOME}"

  . "${HOME}/venv/bin/activate"

  cp -vf -- \
      "${HOME}/default_config.yaml" \
      "${HOME}/.cache/huggingface/accelerate/default_config.yaml" ;

  accelerate launch "${HOME}/docker.infer_qwen.py"
#+end_src

**** Copy the inference script into docker
#+begin_src conf :tangle ./Dockerfile
  COPY './docker.infer_qwen.py' '/root/docker.infer_qwen.py'
  COPY './docker.infer_qwen.sh' '/root/docker.infer_qwen.sh'
#+end_src

*** FLUX

**** python
#+begin_src python :shebang #!/usr/bin/python3 :results output :tangle ./docker.infer_flux.py
  import torch
  from diffusers import FluxPipeline

  pipe = FluxPipeline.from_pretrained(
      "black-forest-labs/FLUX.1-dev", device_map="balanced", torch_dtype=torch.bfloat16
  )

  pipe.transformer = torch.compile(pipe.transformer)


  def do_infer(prompt, path_image_output, width=1360, height=768):
      out = pipe(
          prompt=prompt,
          guidance_scale=3.5,
          height=height,
          width=width,
          num_inference_steps=20,
      ).images[0]

      out.save(path_image_output)


  prompt = "a tiny astronaut hatching from an egg on the moon"
  do_infer(
      prompt=prompt, path_image_output="/data/output/image.png", width=1360, height=768
  )
#+end_src

**** shell
#+begin_src sh :shebang #!/bin/sh :results output :tangle ./docker.infer_flux.sh
  cd "${HOME}"

  . "${HOME}/venv/bin/activate"

  cp -vf -- \
      "${HOME}/default_config.yaml" \
      "${HOME}/.cache/huggingface/accelerate/default_config.yaml" ;

  accelerate launch "${HOME}/docker.infer_flux.py"
#+end_src

**** Copy the inference script into docker
#+begin_src conf :tangle ./Dockerfile
  COPY './docker.infer_flux.py' '/root/docker.infer_flux.py'
  COPY './docker.infer_flux.sh' '/root/docker.infer_flux.sh'
#+end_src

* Molmo inference

** python part
#+begin_src python :shebang #!/usr/bin/python3 :results output :tangle ./docker.infer_molmo.py
  from PIL import Image
  from transformers import AutoModelForCausalLM
  from transformers import AutoProcessor
  from transformers import GenerationConfig
  import requests
  import torch


  class image_loader:
      def __init__(self, path_file_image_input):
          self.o1_image = Image.open(path_file_image_input)


  class molmo_model:
      def __init__(self):
          all_models = (
              "allenai/Molmo-72B-0924",
              "allenai/Molmo-7B-D-0924",
              "allenai/Molmo-7B-O-0924",
          )
          self.processor = AutoProcessor.from_pretrained(
              all_models[1],
              # "allenai/Molmo-72B-0924",
              trust_remote_code=True,
              torch_dtype="auto",
              device_map="auto",
          )

          self.model = AutoModelForCausalLM.from_pretrained(
              "allenai/Molmo-7B-D-0924",
              # "allenai/Molmo-72B-0924",
              trust_remote_code=True,
              torch_dtype="auto",
              device_map="auto",
          )

      def infer(self, image_PIL):
          # process the image and text
          inputs = self.processor.process(
              images=image_PIL,
              text="Describe this image in full detail.",
          )

          # move inputs to the correct device and make a batch of size 1
          inputs = {k: v.to(self.model.device).unsqueeze(0) for k, v in inputs.items()}

          # generate output; maximum 200 new tokens; stop generation when <|endoftext|> is generated
          output = self.model.generate_from_batch(
              inputs,
              GenerationConfig(max_new_tokens=200, stop_strings="<|endoftext|>"),
              tokenizer=self.processor.tokenizer,
          )

          # only get generated tokens; decode them to text
          generated_tokens = output[0, inputs["input_ids"].size(1) :]
          generated_text = self.processor.tokenizer.decode(
              generated_tokens, skip_special_tokens=True
          )

          # print the generated text
          print(generated_text)

          # >>> This image features an adorable black Labrador puppy sitting on a wooden deck.
          #     The puppy is positioned in the center of the frame, looking up at the camera...


  image = image_loader("/data/input/image.png")
  main_model = molmo_model()
  main_model.infer(image.o1_image)
#+end_src

** shell part
#+begin_src sh :shebang #!/bin/sh :results output :tangle ./docker.infer_molmo.sh
  cd "${HOME}"

  . "${HOME}/venv/bin/activate"

  cp -vf -- \
      "${HOME}/default_config.yaml" \
      "${HOME}/.cache/huggingface/accelerate/default_config.yaml" ;

  accelerate launch "${HOME}/docker.infer_molmo.py"
#+end_src

** Copy scripts into the image
#+begin_src conf :tangle ./Dockerfile
  COPY './docker.infer_molmo.py' '/root/docker.infer_molmo.py'
  COPY './docker.infer_molmo.sh' '/root/docker.infer_molmo.sh'
#+end_src

* Final Image

** The image declaration
#+begin_src conf :tangle ./Dockerfile
  FROM cuda_deb_uv_pip_flash_jupyter_inference
#+end_src

* Main script wrapper for docker image name and command
#+begin_src sh :shebang #!/bin/sh :results output :tangle ./host.docker_run_interactive.sh
  "${IMAGE_NAME}"
  '/bin/bash' ;
  # '/root/docker.start_jupyter_lab.sh' ;
#+end_src
